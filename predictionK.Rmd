---
title: "HUDK4051: Prediction - Comparing Trees"
author: "Charles Lang"
date: "1/9/2018"
output: html_document
---

In this assignment you will modelling student data using three flavors of tree algorithm: CART, C4.5 and C5.0. We will be using these algorithms to attempt to predict which students drop out of courses. Many universities have a problem with students over-enrolling in courses at the beginning of semester and then dropping most of them as the make decisions about which classes to attend. This makes it difficult to plan for the semester and allocate resources. However, schools don't want to restrict the choice of their students. One solution is to create predictions of which students are likley to drop out of which courses and use these predictions to inform semester planning. 

In this assignment we will be using the tree algorithms to build models of which students are likely to drop out of which classes. 

## Software

In order to generate our models we will need several packages. The first package you should install is [caret](https://cran.r-project.org/web/packages/caret/index.html).

There are many prediction packages available and they all have slightly different syntax. caret is a package that brings all the different algorithms under one hood using the same syntax. 

We will also be accessing an algorithm from the [Weka suite](https://www.cs.waikato.ac.nz/~ml/weka/). Weka is a collection of machine learning algorithms that have been implemented in Java and made freely available by the University of Waikato in New Zealand. To access these algorithms you will need to first install both the [Java Runtime Environment (JRE) and Java Development Kit](http://www.oracle.com/technetwork/java/javase/downloads/jre9-downloads-3848532.html) on your machine. You can then then install the [RWeka](https://cran.r-project.org/web/packages/RWeka/index.html) package within R.

**Weka requires Java and Java causes problems. If you cannot install Java and make Weka work, please follow the alternative instructions at line 121**
(Issue 1: failure to install RWeka/RWekajars, paste "sudo R CMD javareconf" into terminal and try to install again)

The last package you will need is [C50](https://cran.r-project.org/web/packages/C50/index.html).

## Data

The data comes from a university registrar's office. The code book for the variables are available in the file code-book.txt. Examine the variables and their definitions.

Upload the drop-out.csv data into R as a data frame. 

```{r}
library(plyr)
library(C50)
library(party)
library(caret)
library(RWeka)
library(MLmetrics)
## Data Import 
student <- read.csv('/Users/prediction-master/drop-out.csv')
summary(student) ## No missing values

```

The next step is to separate your data set into a training set and a test set. Randomly select 25% of the students to be the test data set and leave the remaining 75% for your training data set. (Hint: each row represents an answer, not a single student.)

```{r}
## Data partition
trainData <- createDataPartition(
  y=student$complete, p = 0.75, list=FALSE
)
training <- student[trainData,]
testing <- student[-trainData,]
training<-training[,-1]
testing<-testing[,-1]

```

For this assignment you will be predicting the student level variable "complete". 
(Hint: make sure you understand the increments of each of your chosen variables, this will impact your tree construction)

Visualize the relationships between your chosen variables as a scatterplot matrix.  Save your image as a .pdf named scatterplot_matrix.pdf. Based on this visualization do you see any patterns of interest? Why or why not?

```{r}
## Scatter plot
student_noid <- student[,-1]
pairs(~.,data=student_noid)
#pdf(file = "/Users/My_Plot.pdf")
## Since the response variable is non-numeric, the scatter cann't provide 
## meaning full information in variable selection
```

## CART Trees

You will use the [rpart package](https://cran.r-project.org/web/packages/rpart/rpart.pdf) to generate CART tree models.

Construct a classification tree that predicts complete using the caret package.

```{r}
library(caret)
MySummary  <- function(data, lev = NULL, model = NULL){
  df <- defaultSummary(data, lev, model)
  tc <- twoClassSummary(data, lev, model)
  pr <- prSummary(data, lev, model)
  out <- c(df,tc,pr)
  out}

## K fold validation 
ctrl <- trainControl(method = 'repeatedcv',repeats = 3,
                     classProbs = TRUE,
                     summaryFunction = MySummary)
## Model training
fit1 <- train(complete~.,data=training,
              method="rpart",
              preProc=c("center",'scale'),
              trControl =ctrl,
              metric="Accuracy"
              )
fit1$bestTune ## the best tunning in this case is when cp = 0.01005
fit1 ## It is good model fit, the overall accuracy is about 0.89, also a good specificity and an acceptable specificity
sens=0.6553508
spec=0.9951672
2*(sens*spec)/(sens+spec) ## when cp=0.01005, F1 score 

```

Describe important model attribues of your tree. Do you believe it is a successful model of student performance, why/why not?
 It is good model fit, the overall accuracy is about 0.89, also a good specificity and an acceptable specificity
Can you use the sensitivity and specificity metrics to calculate the F1 metric?
2*(sens*spec)/(sens+spec) ## when cp=0.01005, F1 score 

Now predict results from the test data and describe important attributes of this test. Do you believe it is a successful model of student performance, why/why not?
It is a good model, since the accuracy is about 0.91
```{r}
## predict for the test dataset
cartClasses <- predict(fit1,newdata = testing) ## prediction result
confusionMatrix(data = cartClasses, as.factor(testing$complete)) #confusion Matrix


```

## Conditional Inference Trees

Train a Conditional Inference Tree using the `party` package on the same training data and examine your results.
```{r}
## Conditional Inference Trees
conFit<-train(complete~.,data=training,
              method="cforest",
              preProc=c("center",'scale'),
              trControl =ctrl,
              metric="Accuracy"
)
conFit
fit1$finalModel
```
Describe important model attribues of your tree. Do you believe it is a successful model of student performance, why/why not?
the most important variable is years to decide whether a student will drop out.
What does the plot represent? What information does this plot tell us?

Now test your new Conditional Inference model by predicting the test data and generating model fit statistics.
```{r}
## predict for the test dataset
cartClasses <- predict(conFit,newdata = testing) ## prediction result
confusionMatrix(data = cartClasses, as.factor(testing$complete)) # the accuracy is about 0.90
```

There is an updated version of the C4.5 model called C5.0, it is implemented in the C50 package. What improvements have been made to the newer version? 

Install the C50 package, train and then test the C5.0 model on the same data.

```{r}
## Conditional Inference Trees
c50Fit<-train(complete~.,data=training,
              method="C5.0Cost",
              preProc=c("center",'scale'),
              trControl =ctrl,
              metric="Accuracy"
)
c50Fit
```

## Compare the models

caret allows us to compare all three models at once.

```{r}
## summary of the model
resamps <- resamples(list(cart = fit1, condinf = conFit, cfiveo = c50Fit))
summary(resamps)

conFit$finalModel
c50Fit$finalModel
## after checking the three final models, the most important variable is years to decide whether a student will drop out.

```

What does the model summary tell us? Which model do you believe is the best?
It is really hard to say which one is the best model.When comparing the three models, the overall accuracies are pretty close about 0.90, the senstivity for these three models are not very high which are about 0.65,the specificity for these three models are very high which are about 0.99 
Which variables (features) within your chosen model are important, do these features provide insights that may be useful in solving the problem of students dropping out of courses?
years and course_id. Yes, these two features are most important in seperating the dataset